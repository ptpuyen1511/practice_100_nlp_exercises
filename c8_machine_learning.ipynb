{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chương 8: Machine Learning\n",
    "Chương này yêu cầu bạn thực hiện bài toán sentiment analysis trên corpus sentence polarity dataset v1.0 trong Moview Review Data của tác giả Bo Pang và Lillian Lee. Yêu cầu của bài toán sentiment analysis là phân loại các câu thành positive và negative sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70. Download và tiền xử lý dữ liệu\n",
    "Sử dụng dữ liệu liên quan đến sentiment polarity của các câu (download tại đây), tạo dữ liệu chuẩn hoá (sentiment.txt) theo hướng dẫn dưới đây.\n",
    "\n",
    "1. Thêm vào '+1' ở bắt đầu các dòng trong file rt-polarity.pos (giữa +1 và nội dung của câu cách nhau bởi ký tự trắng).\n",
    "\n",
    "2. Thêm vào '-1' ở bắt đầu các dòng trong file rt-polarity.neg (giữa -1 và nội dung của câu cách nhau bởi ký tự trắng).\n",
    "\n",
    "3. Kết hợp nội dung thu được trong phần 1 và 2 để tạo thành file sentiment.txt\n",
    "\n",
    "Sau khi đã thu được file sentiment.txt, xác nhận số lượng các câu với positive polarity và các câu với negative polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "with open('rt-polaritydata/rt-polaritydata/rt-polarity.pos', 'r') as f:\n",
    "    pos = f.readlines()\n",
    "    \n",
    "with open('rt-polaritydata/rt-polaritydata/rt-polarity.neg', 'r') as f:\n",
    "    neg = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment.txt\n",
    "with open('sentiment.txt', 'w') as f:\n",
    "    for line in pos:\n",
    "        f.write('+1 ' + line)\n",
    "    \n",
    "    for line in neg:\n",
    "        f.write('-1 ' + line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "Pos sentence: +1 the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "Neg sentence: -1 enigma is well-made , but it's just too dry and too placid . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check sentiment.txt\n",
    "with open('sentiment.txt', 'r') as f:\n",
    "    sentiment_data = f.readlines()\n",
    "    \n",
    "print(len(sentiment_data))\n",
    "print('Pos sentence: ' + sentiment_data[0])\n",
    "print('Neg sentence: ' + sentiment_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 71. Stopwords\n",
    "Tạo ra danh sách các stopwords trong tiếng Anh. Sau đó viết 1 hàm để kiểm tra một từ có thuộc danh sách stopwords hay không. Hàm sẽ trả về giá trị TRUE nếu từ cho trước thuộc danh sách stopwords. Ngược lại hàm sẽ trả về giá trị FALSE. Sau đó viết mô tả về các test cho hàm đã viết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stopword(word):\n",
    "    \"\"\"\n",
    "    The function is used to check if a word is stopword\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "        Word that we want to check, e.g., ``he`` or ``eat``.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : bool\n",
    "        True if word in stopwords list.\n",
    "        False if word not in stopwords list.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> is_stopword('eat')\n",
    "    False\n",
    "    \n",
    "    >>> is_stopword('both')\n",
    "    True\n",
    "    \n",
    "    >>> is_stopword(1.5)\n",
    "    False\n",
    "    \n",
    "    >>> s = (2,2)\n",
    "    >>> is_stopword(s)\n",
    "    False\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return False if word is not str\n",
    "    if not isinstance(word, str):\n",
    "        return False\n",
    "    \n",
    "    stopwords = {\"both\", \"only\", \"wouldn\", \"against\", \"their\", \"now\", \"didn\", \"himself\", \"ma\", \"yours\", \"having\", \"me\", \"doesn\", \"needn\", \"most\", \"itself\", \"m\", \"s\", \"isn't\", \"shan\", \"did\", \"won't\", \"don\", \"mightn't\", \"where\", \"but\", \"when\", \"wasn't\", \"wouldn't\", \"who\", \"those\", \"more\", \"with\", \"and\", \"whom\", \"an\", \"into\", \"before\", \"you've\", \"it\", \"ve\", \"ain\", \"haven\", \"o\", \"some\", \"are\", \"doesn't\", \"few\", \"she\", \"then\", \"can\", \"will\", \"each\", \"myself\", \"than\", \"has\", \"they\", \"it's\", \"there\", \"hasn\", \"which\", \"until\", \"or\", \"out\", \"re\", \"on\", \"had\", \"your\", \"am\", \"have\", \"in\", \"under\", \"should\", \"been\", \"because\", \"ourselves\", \"shouldn't\", \"that\", \"too\", \"the\", \"from\", \"didn't\", \"you'll\", \"you\", \"haven't\", \"don't\", \"mustn\", \"hadn\", \"own\", \"during\", \"does\", \"his\", \"needn't\", \"by\", \"doing\", \"mustn't\", \"wasn\", \"ll\", \"theirs\", \"other\", \"you're\", \"if\", \"my\", \"over\", \"hasn't\", \"further\", \"above\", \"down\", \"again\", \"why\", \"how\", \"i\", \"its\", \"them\", \"weren't\", \"do\", \"themselves\", \"between\", \"through\", \"here\", \"weren\", \"this\", \"won\", \"isn\", \"all\", \"him\", \"while\", \"for\", \"yourselves\", \"were\", \"to\", \"you'd\", \"shouldn\", \"below\", \"very\", \"couldn\", \"about\", \"she's\", \"off\", \"her\", \"we\", \"d\", \"aren't\", \"just\", \"what\", \"yourself\", \"any\", \"shan't\", \"was\", \"be\", \"nor\", \"t\", \"y\", \"so\", \"hadn't\", \"a\", \"is\", \"couldn't\", \"that'll\", \"our\", \"after\", \"as\", \"he\", \"hers\", \"such\", \"once\", \"aren\", \"these\", \"herself\", \"of\", \"up\", \"same\", \"being\", \"mightn\", \"ours\", \"at\", \"not\", \"no\", \"should've\"}\n",
    "    \n",
    "    return word in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_stopword('eat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_stopword('both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_stopword(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = (2,2)\n",
    "is_stopword(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 72. Trích xuất đặc trưng\n",
    "Tự thiết kế các đặc trưng cho bài toán sentiment analysis. Sau đó trích xuất đặc trưng từ dữ liệu training.\n",
    "\n",
    "Hint: phương pháp trích xuất đặc trưng đơn giản nhất là sử dụng từ gốc (stem) các từ không trong danh sách các stopwords. Phương pháp này có thể sử dụng để làm hệ thống baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[toback's] fondness for fancy split-screen , s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's nice to see piscopo again after all these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a wonderfully warm human drama that remains vi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>with very little to add beyond the dark vision...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a derivative collection of horror and sci-fi c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label\n",
       "0  [toback's] fondness for fancy split-screen , s...      0\n",
       "1  it's nice to see piscopo again after all these...      1\n",
       "2  a wonderfully warm human drama that remains vi...      1\n",
       "3  with very little to add beyond the dark vision...      0\n",
       "4  a derivative collection of horror and sci-fi c...      0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sentiment.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "random.shuffle(lines)\n",
    "    \n",
    "label = [1 if s.split(' ', 1)[0] == '+1' else 0 for s in lines]\n",
    "text = [s.strip().split(' ', 1)[1] for s in lines]\n",
    "\n",
    "data_df = pd.DataFrame(list(zip(text, label)), columns=['comment', 'label'])\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "data_df['simple_preprocessed_comment'] = [simple_preprocess(comment, deacc=True) for comment in data_df['comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>simple_preprocessed_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[toback's] fondness for fancy split-screen , s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[toback, fondness, for, fancy, split, screen, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's nice to see piscopo again after all these...</td>\n",
       "      <td>1</td>\n",
       "      <td>[it, nice, to, see, piscopo, again, after, all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a wonderfully warm human drama that remains vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[wonderfully, warm, human, drama, that, remain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>with very little to add beyond the dark vision...</td>\n",
       "      <td>0</td>\n",
       "      <td>[with, very, little, to, add, beyond, the, dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a derivative collection of horror and sci-fi c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[derivative, collection, of, horror, and, sci,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label  \\\n",
       "0  [toback's] fondness for fancy split-screen , s...      0   \n",
       "1  it's nice to see piscopo again after all these...      1   \n",
       "2  a wonderfully warm human drama that remains vi...      1   \n",
       "3  with very little to add beyond the dark vision...      0   \n",
       "4  a derivative collection of horror and sci-fi c...      0   \n",
       "\n",
       "                         simple_preprocessed_comment  \n",
       "0  [toback, fondness, for, fancy, split, screen, ...  \n",
       "1  [it, nice, to, see, piscopo, again, after, all...  \n",
       "2  [wonderfully, warm, human, drama, that, remain...  \n",
       "3  [with, very, little, to, add, beyond, the, dar...  \n",
       "4  [derivative, collection, of, horror, and, sci,...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [toback, fondness, fancy, split, screen, stutt...\n",
       "1    [nice, see, piscopo, years, chaykin, headly, p...\n",
       "2    [wonderfully, warm, human, drama, remains, viv...\n",
       "3    [little, add, beyond, dark, visions, already, ...\n",
       "4    [derivative, collection, horror, sci, fi, clic...\n",
       "Name: rmstopword_comment, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['rmstopword_comment'] = [[word for word in list_tokens if is_stopword(word) == False] for list_tokens in data_df['simple_preprocessed_comment']]\n",
    "\n",
    "data_df.head()['rmstopword_comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [toback, fond, fanci, split, screen, stutter, ...\n",
       "1    [nice, see, piscopo, year, chaykin, headli, pr...\n",
       "2    [wonder, warm, human, drama, remain, vividli, ...\n",
       "3    [littl, add, beyond, dark, vision, alreadi, re...\n",
       "4             [deriv, collect, horror, sci, fi, clich]\n",
       "Name: rmstopword_stemming_comment, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "poster = PorterStemmer()\n",
    "\n",
    "data_df['rmstopword_stemming_comment'] = [[poster.stem(word) for word in list_tokens] for list_tokens in data_df['rmstopword_comment']]\n",
    "\n",
    "data_df.head()['rmstopword_stemming_comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_df['rmstopword_comment'].apply(lambda x: ' '.join(x))\n",
    "y = data_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(min_df=2, ngram_range=(1, 1))\n",
    "\n",
    "X_train_count_vect = count_vect.fit(X_train).transform(X_train) \n",
    "X_test_count_vect = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "away 1\n",
      "characters 1\n",
      "coming 1\n",
      "everyone 1\n",
      "except 1\n",
      "inducing 1\n",
      "mile 1\n",
      "see 1\n",
      "single 1\n",
      "sleep 1\n",
      "thriller 1\n",
      "twist 1\n"
     ]
    }
   ],
   "source": [
    "# Print score of sentence 0\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "feature_index = X_train_count_vect[0,:].nonzero()[1]\n",
    "count_scores = zip(feature_index, [X_train_count_vect[0, x] for x in feature_index])\n",
    "\n",
    "for w, s in [(feature_names[i], s) for (i, s) in count_scores]:\n",
    "    print(w, s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
